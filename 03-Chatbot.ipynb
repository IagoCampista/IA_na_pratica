{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "add2bd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bibliotecas importadas com sucesso!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iagocampista/anaconda3/envs/travelAgent/lib/python3.11/site-packages/langchain_pinecone/__init__.py:3: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_pinecone.vectorstores import Pinecone, PineconeVectorStore\n"
     ]
    }
   ],
   "source": [
    "# C√âLULA 1\n",
    "\n",
    "# Se precisar, descomente e execute esta linha para instalar as depend√™ncias\n",
    "# !pip install langchain langchain-groq langchain-pinecone sentence-transformers torch python-dotenv ipywidgets\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from operator import itemgetter\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Imports do LangChain\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableBranch, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "print(\"Bibliotecas importadas com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "510a3545",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3x/j5lxxqwd671dhlhn57rtnkyr0000gn/T/ipykernel_39723/1638079777.py:11: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistente pronto para come√ßar! Execute a pr√≥xima c√©lula para iniciar o chat.\n"
     ]
    }
   ],
   "source": [
    "# C√âLULA 2\n",
    "\n",
    "# --- Carregar Vari√°veis de Ambiente ---\n",
    "load_dotenv()\n",
    "\n",
    "# --- Inicializa√ß√£o do LLM e Retriever ---\n",
    "llm = ChatGroq(model_name=\"llama-3.3-70b-versatile\", temperature=0)\n",
    "model_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "# Inicializar o Retriever para buscar informa√ß√µes no Pinecone\n",
    "index_name = \"assistente-viagem\"\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'}\n",
    ")\n",
    "vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# --- Defini√ß√£o dos Prompts ---\n",
    "itinerary_prompt = PromptTemplate(    \n",
    "    template=\"\"\"Voc√™ √© um planejador de viagens experiente.\n",
    "    Com base na PERGUNTA do usu√°rio e no CONTEXTO de informa√ß√µes tur√≠sticas, crie um roteiro de viagem detalhado.\n",
    "    O roteiro deve ser claro, organizado por dias e per√≠odos (manh√£, tarde, noite) e incluir sugest√µes do contexto.\n",
    "    Se n√£o for poss√≠vel formular uma resposta com as informa√ß√µes do contexto, diga que voc√™ n√£o sabe. N√£o invente.\n",
    "\n",
    "    CONTEXTO: {context}\n",
    "    PERGUNTA: {question}\n",
    "    ROTEIRO DETALHADO:\"\"\",\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "\n",
    "logistics_prompt = PromptTemplate(\n",
    "    template=\"\"\"Voc√™ √© um especialista em log√≠stica de viagens.\n",
    "    Responda √† PERGUNTA do usu√°rio focando em transporte, dire√ß√µes e aspectos pr√°ticos.\n",
    "    Use o CONTEXTO para basear sua resposta.\n",
    "    Se a informa√ß√£o n√£o estiver no contexto, diga que voc√™ n√£o sabe. N√£o invente.\n",
    "\n",
    "    CONTEXTO: {context}\n",
    "    PERGUNTA: {question}\n",
    "    RESPOSTA:\"\"\",\n",
    "    input_variables=[\"context\", \"question\"],\n",
    ")\n",
    "\n",
    "local_info_prompt = PromptTemplate(\n",
    "    template=\"\"\"Voc√™ √© um assistente de viagens muito prestativo.\n",
    "    Use o CONTEXTO fornecido para responder √† PERGUNTA do usu√°rio.\n",
    "    Se a informa√ß√£o n√£o estiver no contexto, diga que voc√™ n√£o sabe. N√£o invente.\n",
    "\n",
    "    CONTEXTO: {context}\n",
    "    PERGUNTA: {question}\n",
    "    RESPOSTA:\"\"\",\n",
    "    input_variables=[\"context\", \"question\"],\n",
    ") \n",
    "translation_prompt = PromptTemplate(\n",
    "    template=\"\"\"Voc√™ √© um guia de tradu√ß√£o para turistas.\n",
    "    O usu√°rio quer saber frases √∫teis em outro idioma.\n",
    "    Traduza a seguinte frase ou pedido para o idioma solicitado e forne√ßa uma pron√∫ncia simplificada.\n",
    "\n",
    "    PEDIDO DO USU√ÅRIO: {question}\n",
    "    TRADU√á√ÉO E GUIA DE PRON√öNCIA:\"\"\",\n",
    "    input_variables=[\"question\"]\n",
    ") \n",
    "router_prompt_template = \"\"\"Dada a pergunta do usu√°rio, classifique-a em uma das seguintes categorias.\n",
    "Responda APENAS com o nome da categoria.\n",
    "\n",
    "Categorias:\n",
    "- roteiro-viagem: O usu√°rio est√° pedindo um itiner√°rio, plano de viagem ou sugest√µes de atividades para m√∫ltiplos dias.\n",
    "- logistica-transporte: O usu√°rio est√° perguntando sobre meios de transporte, como chegar a um lugar, aeroportos ou locomo√ß√£o.\n",
    "- info-local: O usu√°rio est√° pedindo informa√ß√µes espec√≠ficas sobre um ponto tur√≠stico, restaurante, hor√°rio de funcionamento, etc.\n",
    "- traducao-idiomas: O usu√°rio est√° pedindo para traduzir uma frase ou palavra.\n",
    "- geral: O usu√°rio est√° fazendo uma pergunta geral que n√£o se encaixa nas outras categorias.\n",
    "\n",
    "Pergunta do usu√°rio:\n",
    "{question}\n",
    "\n",
    "Classifica√ß√£o:\"\"\"\n",
    "\n",
    "# --- Defini√ß√£o das Cadeias Especializadas ---\n",
    "itinerary_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
    "    | itinerary_prompt | llm | StrOutputParser()\n",
    ")\n",
    "logistics_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
    "    | logistics_prompt | llm | StrOutputParser()\n",
    ")\n",
    "local_info_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
    "    | local_info_prompt | llm | StrOutputParser()\n",
    ")\n",
    "translation_chain = (\n",
    "    # O itemgetter aqui garante que a entrada no formato de dicion√°rio tamb√©m funcione\n",
    "    itemgetter(\"question\") \n",
    "    | translation_prompt \n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "# --- Defini√ß√£o do Roteador (Classifier + RunnableBranch) ---\n",
    "#prompt_router = PromptTemplate(template=router_prompt_template, input_variables=[\"question\"])\n",
    "prompt_router = PromptTemplate(\n",
    "    template=router_prompt_template,\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "classifier_chain = prompt_router | llm | StrOutputParser()\n",
    "\n",
    "branch = RunnableBranch(\n",
    "    (lambda x: \"roteiro-viagem\" in x[\"topic\"].lower(), itinerary_chain),\n",
    "    (lambda x: \"logistica-transporte\" in x[\"topic\"].lower(), logistics_chain),\n",
    "    (lambda x: \"info-local\" in x[\"topic\"].lower(), local_info_chain),\n",
    "    (lambda x: \"traducao-idiomas\" in x[\"topic\"].lower(), translation_chain),\n",
    "    llm,\n",
    ")\n",
    "\n",
    "chain = {\n",
    "    \"topic\": (lambda x: {\"question\": x[\"input\"]}) | classifier_chain,\n",
    "    \"question\": lambda x: x[\"input\"]\n",
    "} | branch\n",
    "\n",
    "print(\"Assistente pronto para come√ßar! Execute a pr√≥xima c√©lula para iniciar o chat.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f9e7bed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### ü§ñ Ol√°! Sou seu assistente de viagens."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Digite sua pergunta abaixo ou 'sair' para encerrar.**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**ü§ñ At√© a pr√≥xima!**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# C√âLULA 3\n",
    "\n",
    "def iniciar_chat():\n",
    "    display(Markdown(\"### ü§ñ Ol√°! Sou seu assistente de viagens.\"))\n",
    "    display(Markdown(\"**Digite sua pergunta abaixo ou 'sair' para encerrar.**\"))\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            query = input(\"Voc√™: \")\n",
    "\n",
    "            if query.lower() in [\"sair\", \"exit\", \"fim\"]:\n",
    "                display(Markdown(\"**ü§ñ At√© a pr√≥xima!**\"))\n",
    "                break\n",
    "            \n",
    "            if not query.strip():\n",
    "                continue\n",
    "\n",
    "            # Roda o classificador para descobrir a rota e exibe\n",
    "            topic = classifier_chain.invoke({\"question\": query})\n",
    "            display(Markdown(f\">>> _Roteador acionou a chain: **{topic}**_\"))\n",
    "\n",
    "            # Roda a cadeia principal para obter a resposta\n",
    "            response = chain.invoke({\"input\": query})\n",
    "            \n",
    "            # Exibe a resposta\n",
    "            display(Markdown(f\"**Assistente:** {response}\"))\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            # Permite interromper o loop com Ctrl+C ou pelo bot√£o \"Stop\" do Jupyter\n",
    "            display(Markdown(\"\\n**ü§ñ Chat encerrado.**\"))\n",
    "            break\n",
    "        except Exception as e:\n",
    "            display(Markdown(f\"**Ocorreu um erro:** {e}\"))\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "# Inicia o chat\n",
    "iniciar_chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "travelAgent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
